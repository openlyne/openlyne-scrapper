# Openlyne Scrapper API

Openlyne Scrapper: Production-ready (hardened) Express + Puppeteer microservice to fetch multiple web pages and return their content (HTML/Text/Markdown) and optional screenshots.

## Features

- Shared persistent headless Chromium (relaunches if disconnected)
- Concurrency limiting (configurable, capped)
- Formats: HTML, Text, Markdown (markdown can be disabled)
- Optional screenshots returned inline as Base64
- Bearer token auth (Authorization header)
- Rate limiting & security headers (helmet)
- Request ID correlation & structured logging (pino)
- Graceful shutdown (closes browser & server)
- Basic tests (Jest + mocked puppeteer)
- Dockerfile for container deployment

## Install

```bash
npm install
```

## Run (Development)

```bash
npm start
# or specify a port
PORT=4000 npm start
```

Server starts on `http://localhost:3000` (or provided PORT).

Development pretty logging is enabled. For production build a Docker image or run with `NODE_ENV=production`.

## Environment Variables

See `.env.example` for full list. Key variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `PORT` | HTTP port | 3000 |
| `API_KEY` / `API_KEYS` | Single or comma-separated API keys | (none = auth disabled) |
| `HEADLESS` | Run puppeteer headless (set `false` to debug) | true |
| `SCRAPE_NAV_TIMEOUT_MS` | Per-page navigation timeout | 45000 |
| `SCRAPE_MAX_CONCURRENCY` | Hard upper bound on parallel pages | 5 |
| `SCRAPE_DEFAULT_CONCURRENCY` | Default if not provided in request | 3 |
| `ALLOW_MARKDOWN` | Enable markdown conversion | true |
| `SCREENSHOT_DIR` | (Deprecated) Previously used for local screenshot files | screenshots |
| `RATE_LIMIT_WINDOW_MS` | Rate limit window size | 60000 |
| `RATE_LIMIT_MAX` | Max requests per window per IP | 60 |
| `CORS_ALLOW_ORIGINS` | Comma-separated whitelist (empty = allow all) | (empty) |
| `LOG_LEVEL` | pino log level | info |

## Authentication

Configure one of:

```bash
API_KEY=secret123
# or
API_KEYS=key1,key2,key3
```

Client sends header:

```text
Authorization: Bearer secret123
```

Open endpoints: `/`, `/health`. All others require a valid bearer token when keys configured.

## API Guide

### Overview

Base URL (local dev): `http://localhost:3000`

All responses are JSON. Errors follow the shape:

```json
{ "error": "Message" }
```

### Common Headers

| Header | Purpose |
|--------|---------|
| `Authorization: Bearer <token>` | Required for protected endpoints when auth enabled |
| `Content-Type: application/json` | For POST bodies |
| `x-request-id` | (Response) Correlation ID generated by server |

### Endpoints

#### 1. GET /

Health/info landing.

Response 200:

```json
{ "status": "ok", "message": "Openlyne Scrapper API. POST /scrape { urls: [...] }" }
```

#### 2. GET /health

Simple liveness probe.

Response 200:

```json
{ "status": "ok" }
```

#### 3. POST /scrape

Scrape one or more pages and optionally capture a screenshot.

Request JSON body:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `urls` | string[] | yes | HTTP/HTTPS URLs to fetch |
| `screenshot` | boolean | no | Capture screenshot (default false) |
| `concurrency` | number | no | Parallel pages (bounded) |
| `format` | 'html' \| 'text' \| 'markdown' | no | Content extraction strategy (default html) |
| `clean` | boolean | no | Deprecated alias for format='text' when true & no format provided |

Successful response 200:

```json
{
  "results": [
    {
      "url": "https://example.com/",
      "elapsedSeconds": 1.237,
      "content": "Example Domain...",
      "contentFormat": "text",
  "screenshotBase64": "iVBORw0KGgoAAAANSUhEUgAA..."
    }
  ],
  "meta": { "count": 1, "durationMs": 1250, "durationSeconds": 1.25 }
}
```

Result item fields:

| Field | Description |
|-------|-------------|
| `url` | Normalized URL requested |
| `elapsedSeconds` | Duration for that page |
| `content` | Extracted page content (omitted on failure) |
| `contentFormat` | Echo of requested format |
| `screenshotBase64` | Base64 encoded PNG screenshot (when screenshot=true) |
| `error` | Error message if page failed |

Meta fields:

| Field | Description |
|-------|-------------|
| `count` | Number of input URLs |
| `durationMs` | Total batch duration in ms |
| `durationSeconds` | Total batch duration in seconds (3dp) |

### HTTP Status Codes

| Code | When |
|------|------|
| 200 | Success |
| 400 | Invalid input (bad URL, unsupported format, empty list) |
| 401 | Missing/invalid bearer token when auth enabled |
| 429 | Rate limit exceeded |
| 500 | Unexpected server error |

### Validation Rules

- `urls` must be a non-empty array.
- Only `http` and `https` protocols allowed.
- `concurrency` coerced to number; below 1 → default; above cap → cap.
- `markdown` disabled if `ALLOW_MARKDOWN=false`.

### Concurrency & Performance

- Each request uses a shared browser; new pages are opened per URL.
- Adjust `SCRAPE_MAX_CONCURRENCY` cautiously—higher values can trigger anti-bot measures.

### Screenshot Behavior

When `screenshot=true` a Base64 PNG string (`screenshotBase64`) is returned inline. Large responses can increase payload size—disable if not needed.

### Example (Multiple URLs, Markdown + Screenshot)

```bash
curl -s -X POST http://localhost:3000/scrape \
  -H "Authorization: Bearer $API_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"urls":["https://example.com","https://example.org"],"format":"markdown","screenshot":true}' | jq '.results[] | {url, haveScreenshot:(.screenshotBase64|length>0), error}'
```

### Error Examples

Invalid protocol:

```json
{ "error": "Unsupported protocol in file://etc/passwd" }
```

Missing token:

```json
{ "error": "Unauthorized" }
```

Navigation timeout (per page):

```json
{ "url": "https://slow.example", "elapsedSeconds": 45.001, "error": "Navigation timeout" }
```

### Request ID Correlation

Each response includes `x-request-id`. Provide your own via that header to trace logs end-to-end.

### Rate Limiting

Global in-memory limiter keyed by IP. For multi-instance deployments use a shared store (Redis) to avoid uneven throttling.

### Production Hardening Suggestions

- Add domain allowlist / SSRF protection (e.g. forbid private IP ranges).
- Enforce max response size or drop large pages early.
- Add metrics endpoint (Prometheus) for scrape durations and failures.
- Implement signed URLs instead of public screenshot URLs if privacy required.


## Endpoint

`POST /scrape`

### Request Body

```jsonc
{
  "urls": ["https://example.com", "https://example.org"],
  "screenshot": true,             // optional: false | true (default false)
  "concurrency": 3,               // optional (default 3)
  "format": "text",               // optional: html | text | markdown (default html)
  "clean": true                   // deprecated alias for format=text
}
```

### Response

```jsonc
{
  "results": [
    {
      "url": "https://example.com",
      "elapsedSeconds": 1.237,
      "content": "Example Domain...",     // depends on format
      "contentFormat": "text",
    "screenshotBase64": "iVBORw0KGgoAAAANSUhEUgAA..."
    },
    {
      "url": "https://example.org",
      "elapsedSeconds": 2.101,
      "error": "Navigation timeout"
    }
  ],
  "meta": { "count": 2, "durationMs": 3338, "durationSeconds": 3.338 }
}
```

Fields:

- `elapsedSeconds`: Time in seconds (3 decimal precision).
- `content`: The extracted content. Type depends on `format`.
- `contentFormat`: Echoes the requested format.
- `error`: Error message if scraping failed.
- `screenshotBase64`: Base64 encoded PNG when screenshot requested.

Formats:

- `html`: Full page HTML (original Puppeteer `page.content()`).
- `text`: Visible text only with scripts/styles removed and collapsed whitespace.
- `markdown`: HTML converted to Markdown (uses Turndown; falls back to plain text if module missing).

## Curl Examples

HTML (default):

```bash
curl -s -X POST http://localhost:3000/scrape \
  -H 'Content-Type: application/json' \
  -d '{"urls":["https://example.com"]}' | jq '.results[0].contentFormat,.results[0].elapsedSeconds'
```

Plain text with screenshot file:

```bash
curl -s -X POST http://localhost:3000/scrape \
  -H 'Content-Type: application/json' \
  -d '{"urls":["https://example.com"],"format":"text","screenshot":true}' | jq '.results[0]'
```

Markdown + screenshot (showing size only):

```bash
curl -s -X POST http://localhost:3000/scrape \
  -H 'Content-Type: application/json' \
  -d '{"urls":["https://example.com"],"format":"markdown","screenshot":true}' | jq '.results[0].screenshotBase64 | ("length=" + (length|tostring))'
```

## Notes

### Architecture

- A shared Puppeteer browser is reused between requests to reduce startup overhead.
- If the browser disconnects it will be lazily recreated on the next scrape.
- Concurrency per request is bounded by `SCRAPE_MAX_CONCURRENCY`.

### Security Considerations

- Always set API keys in production.
- Consider adding domain allowlist logic (not implemented) if you need to prevent SSRF.
- Rate limiting is basic; for multi-instance deployments back it with a shared store (Redis).
- Returned HTML/Text is untrusted content—sanitize if rendering elsewhere.
- Use HTTPS (TLS termination) so bearer tokens are not exposed in transit.
- Prefer short-lived tokens and rotate them periodically; support multiple via `API_KEYS` to allow zero-downtime rotation.
- Avoid logging Authorization headers (current logger does not log headers by default; keep it that way).
- If deploying behind a proxy/load balancer set `app.set('trust proxy', 1)` (add as needed) before rate limiter so IPs are correct.
- Consider adding a maximum response payload size or truncation to mitigate extremely large pages.

### Operational

- Structured logs with request IDs (`x-request-id`).
- Graceful shutdown handles SIGINT/SIGTERM; force exits after 10s.
- Health endpoint: `/health`.

### Testing

Run tests:

```bash
npm test
```

### Docker

Build (production image):

```bash
docker build -t openlyne-scrapper .
```

Run container (exposes 3000):

```bash
docker run --rm -p 3000:3000 -e API_KEY=secret123 openlyne-scrapper
```

Test health:

```bash
curl -s http://localhost:3000/health | jq
```

Test scrape (auth header required if API_KEY set):

```bash
curl -s -X POST http://localhost:3000/scrape \
  -H "Authorization: Bearer secret123" \
  -H "Content-Type: application/json" \
  -d '{"urls":["https://example.com"],"format":"text","screenshot":false}' | jq '.results[0] | {url,elapsedSeconds,contentFormat,(.content|length) as $l | length: $l}'
```

If port 3000 is busy, map a different host port:

```bash
docker run --rm -p 3010:3000 -e API_KEY=secret123 openlyne-scrapper
```

Notes:

- Image installs required Alpine packages for Chromium so Puppeteer works out-of-the-box.
- Runs as a non-root user `pptr` for better security.
- Includes a `HEALTHCHECK` hitting `/health` (Docker marks container unhealthy if endpoint fails).
- Set `LOG_LEVEL=debug` for more verbose logs during diagnosis.
- To reduce image size further you can multi-stage build with `npm ci --omit=dev` and prune locales/fonts.

Example docker-compose service:

```yaml
services:
  openlyne-scrapper:
    build: .
    environment:
      API_KEY: ${API_KEY:-secret123}
      LOG_LEVEL: info
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
```

### Future Improvements

- Add page pooling & queue across requests.
- Optional persistent storage or caching.
- Metrics (Prometheus) endpoint.

### Troubleshooting

If npm peer dependency resolution fails installing ESLint standard config you can fall back to:

```bash
npm install --legacy-peer-deps
```

or skip dev tooling entirely in production builds:

```bash
npm install --omit=dev
```

### Puppeteer Chromium Launch Issues

If you see an error like:

```text
Failed to launch the browser process! spawn /root/.cache/puppeteer/.../chrome ENOENT
```

It means Puppeteer attempted to use a cached/bundled Chromium binary that is not present in the container. This image installs system Chromium via Alpine packages and sets `PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser`.

Fix checklist:

- Ensure the container includes `chromium` (already in Dockerfile).
- Verify env var: `echo $PUPPETEER_EXECUTABLE_PATH` inside container.
- Confirm binary exists: `docker exec <ctr> ls -l /usr/bin/chromium-browser` (or `/usr/bin/chromium`).
- The code resolves a valid path and adds hardened flags (`--no-sandbox`, `--disable-dev-shm-usage`, etc.).

If running in a restricted environment (some PaaS) you may need to add or remove flags:

- Remove `--single-process` (not currently used) if you introduce it and get crashes.
- If sandboxing is supported, you can drop `--no-sandbox` and `--disable-setuid-sandbox` for extra security.

Override executable path manually:

```bash
docker run -e PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium -p 3000:3000 openlyne-scrapper
```

If you still encounter missing shared libraries, install additional packages (example):

```bash
apk add --no-cache libstdc++ mesa-libgbm
```

## License

MIT
